{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<img align=\"left\" height=\"100\" width=\"300\" src=\"./src/readme/logo_white.svg\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# How to generate predictions with Modulos AutoML"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This jupyter notebook describes how to generate predictions with the Modulos AutoML solution. It introduces the online and the batch clients. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**\u26a0\ufe0f Please make sure to install the necessary requirements first. For more information please have look at the README/Installation section.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Online Client"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import online_client as oc"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The online client allows you to perform predictions on single sample data. The online client can also be used for multiple samples or a stream of data by doing sample wise predictions. Instead of providing the data via a .tar file, you will now have to provide the data in the form of a python dictionary.\n", "\n", "**Recommendation:** We recommend using the online client when you have a stream of input data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Example data point (replace with your data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is how the input dictionary should look like for one example data point. You can replace this dictionary with your own data."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "sample_cell"}, "outputs": [], "source": "sample_dict = {\n    'gi': 1.2780000000000022,\n    'gk': 4.847000000000001,\n    'gr': 0.7530000000000001,\n    'gw1': 6.8450000000000015,\n    'gw2': 7.9849999999999985,\n    'gz': 1.7840000000000025,\n    'ij': 1.6529999999999987,\n    'ik': 3.5689999999999986,\n    'iw1': 5.5669999999999975,\n    'iw2': 6.706999999999997,\n    'iz': 0.5060000000000002,\n    'jw1': 3.914,\n    'jw2': 5.053999999999999,\n    'kw1': 1.998,\n    'kw2': 3.137999999999998,\n    'ri': 0.5250000000000021,\n    'rw1': 6.0920000000000005,\n    'rw2': 7.231999999999997,\n    'rz': 1.0310000000000024,\n    'sample_ids_generated': '210',\n    'ug': 1.4840000000000049,\n    'ui': 2.7620000000000084,\n    'uj': 4.415000000000006,\n    'uk': 6.331000000000008,\n    'ur': 2.237000000000005,\n    'uw1': 8.329000000000006,\n    'uw2': 9.469000000000005,\n    'uz': 3.268000000000008,\n    'w1w2': 1.1399999999999988,\n    'zj': 1.1469999999999985,\n    'zk': 3.062999999999999,\n    'zw1': 5.060999999999998,\n    'zw2': 6.200999999999998\n}"}, {"cell_type": "markdown", "metadata": {}, "source": ["### Run online prediction script"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The online client takes a single sample python dictionary as input and outputs predictions in the form of a dictionary. It does so by performing the following steps:\n", "* Creating a temporary directory to save and store intermediate calculations.\n", "* Saving the input python dictionary as HDF5 version of the dataset (this step will likely be removed in a future version).\n", "* Running the feature extractor on the data.\n", "* Running the model to get predictions.\n", "* Transforming the predictions into a dictionary."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["oc.main(sample_dict=sample_dict, tmp_dir=oc.TMP_DIR)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Batch Client"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["import batch_client as bc \n", "import os\n", "import shutil"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The batch client allows you to get predictions for a large number of samples without changing the interface of how you provide the data to the solution. This means, you package your data the same way (in terms of structure) as you did for the data upload. Then, you are ready to go.\n", "\n", "**Recommendation:** We recommend using the batch client if you would like to quickly generate predictions for a full dataset or if you would like to ensure that the solution works correctly. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Set path variables"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* **path_to_tar:** Path to the tar file. The tar file has to contain the same data and has to be packed in the same way as the dataset that was uploaded to the platform. The example dataset provided here was split of the validation set of the original, uploaded dataset. Please replace it with your own dataset to generate new predictions.\n", "* **path_to_tmp:** Path to temporary data folder."]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["path_to_tar = \"src/sample_dataset/generated_dataset.tar\"\n", "path_to_tmp = os.path.join(bc.DEFAULT_OUTPUT_DIR,\"tmp_data_dir\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Run the batch client "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove temporary files from previous run:"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["if os.path.exists(path_to_tmp):\n", "    shutil.rmtree(path_to_tmp)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The batch client takes a `.tar` file as input and outputs predictions in same format as training labels have been. It does so by performing the following steps:\n", "* Creating a temporary directory to save and store intermediate calculations.\n", "* Converting the `.tar` data set into an internal format (HDF5 file). Saved in `path_to_hdf5_data`\n", "* Running the feature extractor on the data.\n", "* Running the model to get predictions.\n", "* Saving the predictions into the same format as training labels have been when training on the platform."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["bc.main(dataset_path=path_to_tar, output_dir_user=\"\", verbose=True,\n", "        keep_tmp=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Look at the predictions"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["from IPython.display import HTML\n", "from modulos_utils.solution_utils import jupyter_utils as ju"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["displayer = ju.JupyterDisplayer.construct(base_dir=bc.FILE_DIR)\n", "HTML(displayer.show())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Clean Up"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Delete the entire output_batch_client folder including all predictions."]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# import shutil\n", "# shutil.rmtree(bc.DEFAULT_OUTPUT_DIR)"]}, {"cell_type": "markdown", "metadata": {"id": "copyright_cell"}, "source": "\u00a9 Modulos AG 2019-2022. All rights reserved."}], "metadata": {"kernelspec": {"display_name": "", "language": "python", "name": ""}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": ""}}, "nbformat": 4, "nbformat_minor": 4}